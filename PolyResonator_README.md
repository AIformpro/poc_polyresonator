# PolyResonator â€” POC â€œÃ  fondâ€ (Zoran IA)

## ğŸ¯ Objectif
DÃ©montrer lâ€™**orchestration polyphonique** de plusieurs IA simulÃ©es :
- orchestrateur **UCB1** (multi-armed bandit)
- **mixer LoRA-like** (EMA adaptatif)
- **garde Î”M11.3** (rollback si instabilitÃ©)
- mesures de cohÃ©rence/stabilitÃ©/latence/cost

**IdÃ©e clÃ© :** au lieu dâ€™un modÃ¨le isolÃ©, un *rÃ©seau vivant* qui sâ€™auto-corrige et distribue ses forces.

---

## âš™ï¸ Fonctionnement
- **4 modÃ¨les simulÃ©s** : `reasoner`, `coder`, `vision`, `retriever`
- **4 types de tÃ¢ches** : `theorem`, `bugfix`, `diagram`, `research`
- Chaque modÃ¨le gÃ©nÃ¨re un vecteur (6D) â†’ score relatif Ã  la tÃ¢che
- Orchestrateur **UCB1** choisit un modÃ¨le Ã  mettre en avant
- Mixer ajuste les poids progressivement (EMA)
- Fusion des scores pondÃ©rÃ©s â†’ reward global
- Garde Î”M11.3 : rollback si stabilitÃ© < 0.70 (selon cohÃ©rence de phase)

---

## ğŸ“Š MÃ©triques produites
- `reward_avg` â†’ qualitÃ© moyenne des sorties
- `coherence_avg` â†’ cohÃ©rence de phase multi-modÃ¨les
- `stability_avg` â†’ robustesse globale
- `latency_p95` â†’ latence simulÃ©e (95e percentile)
- `cost_total` â†’ coÃ»t simulÃ© ($ ou ressources)
- `rollbacks` â†’ nb de retours forcÃ©s Ã  snapshot

---

## ğŸš€ Utilisation
### Installation
```bash
unzip poc_polyresonator.zip
cd polyresonator
```

### ExÃ©cution simple
```bash
python -m polyresonator.cli --steps 200 --seed 42 --outdir outputs --plot
```

### RÃ©sultats
- `outputs/summary.json` : rÃ©sumÃ© des mÃ©triques
- `outputs/history.csv` : historique complet (reward, cohÃ©rence, latence, poids mixerâ€¦)
- `outputs/reward.png` : courbe reward (si matplotlib dispo)

---

## ğŸ” Exemple de rÃ©sultats (200 steps, seed=42)
- **reward_avg â‰ˆ 0.73** â†’ lâ€™orchestrateur apprend Ã  bien choisir
- **stability_avg â‰ˆ 0.82** / **coherence_avg â‰ˆ 0.65** â†’ systÃ¨me globalement stable
- **latency_p95 â‰ˆ 116 ms** â†’ latence rÃ©aliste simulÃ©e
- **rollbacks = 2** â†’ deux instabilitÃ©s dÃ©tectÃ©es et corrigÃ©es

---

## ğŸ§  InterprÃ©tation
- Le systÃ¨me **apprend dynamiquement** Ã  mieux rÃ©partir lâ€™effort entre modÃ¨les.
- Le **mixer LoRA-like** crÃ©e une polyphonie adaptative.
- Le garde **Î”M11.3** Ã©vite la dÃ©rive â†’ preuve que lâ€™auto-correction est possible.

---

## ğŸ”® Prolongement vers le rÃ©el
Pour passer du POC simulÃ© Ã  la pratique :
1. Remplacer `synth_model_output()` par de vrais modÃ¨les (API GPT/Claude, HF Llama, DeepSeekâ€¦).
2. DÃ©finir des vecteurs de tÃ¢che rÃ©els (benchmarks SWE-bench, AIME, MMMU).
3. Calculer le reward comme une mÃ©trique concrÃ¨te (exactitude, tests unitaires, prÃ©fÃ©rences humaines).
4. Activer Î”M11.3 sur ces vraies mÃ©triques.
5. Monitorer via Prometheus/Grafana â†’ intÃ©gration dans Z-Network Emergence.

---

## ğŸ“œ Licence
MIT â€” open et gratuit, destinÃ© Ã  Ãªtre forkÃ©, Ã©tudiÃ© et amÃ©liorÃ©.
